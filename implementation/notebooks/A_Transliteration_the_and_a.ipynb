{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Transliteration \"the\" and \"a\"\n",
    "---\n",
    "This notebook provides code that specifically handles the transliteration of the words \"the\" and \"a\", as their pronunciation can vary depending on the words that follow them in a sentence. The transliterations for these words are provided in advance for the target languages. To run this code, you need access to the OpenAI API. Visit [OpenAI's website](https://openai.com/index/openai-api/) to purchase the required quotas. Once you have your API credentials, put them in the following cell: your API key (`api_key`) and API Base Link (`api_base`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############### API Key of Elevenlabs ###############\n",
    "#####################################################\n",
    "\n",
    "# api_key = \"sk_...\"\n",
    "# api_base = \"\"\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from phonemizer import phonemize\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import sys\n",
    "sys.path.append('../pyfiles/')\n",
    "from normalizer import EnglishTextNormalizer\n",
    "from postprocessing import get_json_result, CheckResultValidity, PostprocessTransliteration, GetResult\n",
    "from gpt import gpt_api_no_stream, GetLLMPrompt\n",
    "\n",
    "\n",
    "normalizer = EnglishTextNormalizer()\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "adds = {\n",
    "    \"zhi\": [\"the\", [\"ðɪ\"]],\n",
    "    \"za\": [\"the pineapple\", [\"ðə\", \"pˈaɪnæpəl\"]],\n",
    "    \"ah\": [\"a little awkward\", [\"ɐ\",\"lˈɪɾəl\",\"ˈɔːkwɚd\"]],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "# Transliterate Multiple Texts\n",
    "---\n",
    "\n",
    "In this example, we will transliterate multiple English sentences using a GPT model. To improve the reliability of the results, the code generates several transliteration responses for each sentence. Adjust the following variables as needed:\n",
    "\n",
    "- `language`: A string specifying the target language for transliteration. The supported options are \"Hindi\", \"Korean\", and \"Japanese\".\n",
    "- `gptmodel`: A string that indicates which GPT model to use. The available options include \"gpt-3.5\", \"gpt-4omini\", \"gpt-4o\", and \"gpt-o1mini\". You can add or modify the list of models by editing the file `MacST-project-page/pyfiles/gpt.py`.\n",
    "- `savedir` : A string that specifies the directory where all transliteration responses will be saved.\n",
    "- `repeatnum`: An integer that sets the number of responses (transliterations) to generate for each sentence.\n",
    "- `reset_response`: A boolean that determines whether to re-generate the transliteration responses, even if previous responses exist in `savedir`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "language = \"Hindi\"\n",
    "gptmodel = \"gpt-4omini\"\n",
    "savedir = f\"./responses_the_a/{language}/\"\n",
    "repeatnum = 5 # Increase this number for more reliable transliteration\n",
    "reset_response = False\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "sentence_list = {key: adds[key][0] for key in adds}\n",
    "\n",
    "# Save the valid responses\n",
    "for key in sentence_list:\n",
    "    print(key)\n",
    "    exist_length = len(glob.glob(savedir+f\"postprocessing_{key}_*.npy\"))\n",
    "    if not(reset_response) and exist_length>=repeatnum:\n",
    "        continue\n",
    "    sentence = sentence_list[key]\n",
    "    inputtext = normalizer(sentence)\n",
    "    prompt = GetLLMPrompt(inputtext, language, phonemized=adds[key][1])\n",
    "    \n",
    "    for r in tqdm(range(repeatnum)):\n",
    "        savepath = savedir + f\"postprocessing_{key}_{r}.npy\"\n",
    "        if not(reset_response) and os.path.exists(savepath):\n",
    "            continue\n",
    "        result = GetResult(client, prompt, gptmodel, inputtext, normalizer, display_print=False)\n",
    "        os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "        np.save(savepath, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
