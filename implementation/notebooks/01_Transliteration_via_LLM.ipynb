{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Transliteration via Large Language Models (LLMs) \n",
    "---\n",
    "This notebook provides code that transliterates English text using large language models (LLMs), specifically OpenAI's GPT models. To run this code, you need access to the OpenAI API. Visit [OpenAI's website](https://openai.com/index/openai-api/) to purchase the required quotas. Once you have your API credentials, put them in the following cell: your API key (`api_key`) and API Base Link (`api_base`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############### API Key of Elevenlabs ###############\n",
    "#####################################################\n",
    "\n",
    "# api_key = \"sk_...\"\n",
    "# api_base = \"\"\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from phonemizer import phonemize\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import sys\n",
    "sys.path.append('../pyfiles/')\n",
    "from normalizer import EnglishTextNormalizer\n",
    "from postprocessing import get_json_result, CheckResultValidity, PostprocessTransliteration, GetResult\n",
    "from gpt import gpt_api_no_stream, GetLLMPrompt\n",
    "\n",
    "\n",
    "normalizer = EnglishTextNormalizer()\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "#########################################\n",
    "### Get the phonemes of \"the\" and \"a\" ###\n",
    "#########################################\n",
    "\n",
    "adds = {\n",
    "    \"zhi\": [\"the\", [\"ðɪ\"]],\n",
    "    \"za\": [\"the pineapple\", [\"ðə\", \"pˈaɪnæpəl\"]],\n",
    "    \"ah\": [\"a little awkward\", [\"ɐ\",\"lˈɪɾəl\",\"ˈɔːkwɚd\"]],\n",
    "}\n",
    "postprocessing = {a: {} for a in adds}\n",
    "for addname in adds:\n",
    "    sentence, phonemized = adds[addname]\n",
    "    for language in [\"Hindi\", \"Korean\", \"Japanese\"]:\n",
    "        filelists = glob.glob(f\"./responses_the_a/{language}/postprocessing_{addname}_*.npy\")\n",
    "        a_list = [np.load(path, allow_pickle=True).item() for path in filelists]\n",
    "        dirs = []\n",
    "        for a in a_list:\n",
    "            a = {key: a[key] for key in sentence.split()}\n",
    "            dirs += [a]\n",
    "        for i in range(len(dirs)):\n",
    "            for key in dirs[i]:\n",
    "                newlist = []\n",
    "                for j in range(len(dirs[i][key][\"similarity order\"])):\n",
    "                    newlist += [dirs[i][key][\"similarity order\"][j]]*(3-j)\n",
    "                dirs[i][key][\"similarity order\"] = newlist\n",
    "        data = {key: [element for i in range(len(dirs)) for element in dirs[i][key][\"similarity order\"]] for key in dirs[0]}\n",
    "        # Get the transliterated sentences\n",
    "        arrays = []\n",
    "        counts = []\n",
    "        for word in sentence.split():\n",
    "            c = collections.Counter(data[word])\n",
    "            df = pd.DataFrame(c.items(), columns=[\"phonemes\", \"count\"]).sort_values(\"count\", ascending=False).values\n",
    "            arrays += [df[0,0]]\n",
    "            counts += [list(df[:1,1])]\n",
    "            \n",
    "        postprocessing[addname][language] = arrays[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "# Trial of Transliteration via LLMs\n",
    "---\n",
    "\n",
    "In this example, we will transliterate an English sentence using a GPT model. Please adjust the following variables:\n",
    "\n",
    "- `sentence`: A string containing the English sentence you wish to transliterate.\n",
    "- `language`: A string specifying the target language. Supported options are \"Hindi\", \"Korean\", and \"Japanese\".\n",
    "- `gptmodel`: A string indicating which GPT model to use. Available options include \"gpt-3.5\", \"gpt-4omini\", \"gpt-4o\", and \"gpt-o1mini\". You can add or modify the list of released models by editing the file `MacST-project-page/sho_util/pyfiles/gpt.py`.\n",
    "\n",
    "Feel free to try out the transliteration with one response from the GPT model using these variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "sentence = \"Transliterate English text into Hindi text.\"\n",
    "language = \"Hindi\"\n",
    "gptmodel = \"gpt-4omini\"\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "inputtext = normalizer(sentence)\n",
    "prompt = GetLLMPrompt(inputtext, language)\n",
    "result = GetResult(client, prompt, gptmodel, inputtext, normalizer, display_print=True)\n",
    "transliterated = PostprocessTransliteration(sentence, [result], normalizer, adds, postprocessing)\n",
    "\n",
    "print(\"English       :\", sentence)\n",
    "print(\"Normalized    :\", inputtext)\n",
    "print(\"Transliterated:\", transliterated)\n",
    "print(\"\\n----------------------------------------\\n----------------------------------------\\n----------------------------------------\\n\")\n",
    "print(\"PROMPT:\\n\")\n",
    "print(prompt)\n",
    "print(\"\\n----------------------------------------\\n----------------------------------------\\n----------------------------------------\\n\")\n",
    "print(\"Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "# Transliterate Multiple Texts\n",
    "---\n",
    "\n",
    "In this example, we will transliterate multiple English sentences using a GPT model. To improve the reliability of the results, the code generates several transliteration responses for each sentence. Adjust the following variables as needed:\n",
    "\n",
    "- `sentence_list`: A dictionary where each key is a text name and the corresponding value is the English sentence you want to transliterate.\n",
    "- `language`: A string specifying the target language for transliteration. The supported options are \"Hindi\", \"Korean\", and \"Japanese\".\n",
    "- `gptmodel`: A string that indicates which GPT model to use. The available options include \"gpt-3.5\", \"gpt-4omini\", \"gpt-4o\", and \"gpt-o1mini\". You can add or modify the list of models by editing the file `MacST-project-page/sho_util/pyfiles/gpt.py`.\n",
    "- `savedir` : A string that specifies the directory where all transliteration responses will be saved.\n",
    "- `repeatnum`: An integer that sets the number of responses (transliterations) to generate for each sentence.\n",
    "- `reset_response`: A boolean that determines whether to re-generate the transliteration responses, even if previous responses exist in `savedir`.\n",
    "- `transliterated_results`: A dictionary where each key is a text name and the corresponding value is the transliterated text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "sentence_list = {\n",
    "    \"text1\": \"ICASSP in India.\",\n",
    "    \"text2\": \"I'm Sho Inoue.\",\n",
    "}\n",
    "language = \"Hindi\"\n",
    "gptmodel = \"gpt-4omini\"\n",
    "savedir = f\"./responses_{language}_{gptmodel}/\"\n",
    "repeatnum = 3 # Increase this number for more reliable transliteration\n",
    "reset_response = False\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "# Save the valid responses\n",
    "for key in sentence_list:\n",
    "    print(key)\n",
    "    exist_length = len(glob.glob(savedir+f\"{key}_*.npy\"))\n",
    "    if not(reset_response) and exist_length>=repeatnum:\n",
    "        continue\n",
    "    sentence = sentence_list[key]\n",
    "    inputtext = normalizer(sentence)\n",
    "    prompt = GetLLMPrompt(inputtext, language)\n",
    "    \n",
    "    for r in tqdm(range(repeatnum)):\n",
    "        savepath = savedir + f\"{key}_{r}.npy\"\n",
    "        if not(reset_response) and os.path.exists(savepath):\n",
    "            continue\n",
    "        result = GetResult(client, prompt, gptmodel, inputtext, normalizer, display_print=False)\n",
    "        os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "        np.save(savepath, result)\n",
    "\n",
    "transliterated_results = {}\n",
    "for key in sentence_list:\n",
    "    files = glob.glob(savedir+f\"{key}_*.npy\")\n",
    "    transliterated = PostprocessTransliteration(sentence_list[key], [np.load(path, allow_pickle=True).item() for path in files], normalizer, adds, postprocessing)\n",
    "    transliterated_results[key] = transliterated\n",
    "    \n",
    "for key in sentence_list:\n",
    "    print(\"\\n----------------------------------------\\n----------------------------------------\\n----------------------------------------\\n\")\n",
    "    print(\"Key           :\", key)\n",
    "    print(\"English       :\", sentence_list[key])\n",
    "    print(\"Transliterated:\", transliterated_results[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
