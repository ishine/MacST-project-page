{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Transliteration via Large Language Models (LLMs) \n",
    "---\n",
    "This notebook provides code that transliterates English text using large language models (LLMs), specifically OpenAI's GPT models. To run this code, you need access to the OpenAI API. Visit [OpenAI's website](https://openai.com/index/openai-api/) to purchase the required quotas. Once you have your API credentials, put them in the following cell: your API key (`api_key`) and API Base Link (`api_base`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############### API Key of Elevenlabs ###############\n",
    "#####################################################\n",
    "\n",
    "api_key = \"sk_...\"\n",
    "api_base = \"\"\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from phonemizer import phonemize\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../sho_util/pyfiles/')\n",
    "from gpt import gpt_api_no_stream, get_json_result\n",
    "\n",
    "import re\n",
    "from whisper.normalizers.english import EnglishNumberNormalizer, EnglishSpellingNormalizer, remove_symbols_and_diacritics\n",
    "\n",
    "# keep numbers, -, and '\n",
    "class EnglishTextNormalizer:\n",
    "    def __init__(self):\n",
    "        self.ignore_patterns = r\"\\b(hmm|mm|mhm|mmm|uh|um)\\b\"\n",
    "        self.replacers = {\n",
    "            # common contractions\n",
    "            r\"\\bwon't\\b\": \"will not\",\n",
    "            r\"\\bcan't\\b\": \"can not\",\n",
    "            r\"\\blet's\\b\": \"let us\",\n",
    "            r\"\\bain't\\b\": \"aint\",\n",
    "            r\"\\by'all\\b\": \"you all\",\n",
    "            r\"\\bwanna\\b\": \"want to\",\n",
    "            r\"\\bgotta\\b\": \"got to\",\n",
    "            r\"\\bgonna\\b\": \"going to\",\n",
    "            r\"\\bi'ma\\b\": \"i am going to\",\n",
    "            r\"\\bimma\\b\": \"i am going to\",\n",
    "            r\"\\bwoulda\\b\": \"would have\",\n",
    "            r\"\\bcoulda\\b\": \"could have\",\n",
    "            r\"\\bshoulda\\b\": \"should have\",\n",
    "            r\"\\bma'am\\b\": \"madam\",\n",
    "            # contractions in titles/prefixes\n",
    "            r\"\\bmr\\b\": \"mister \",\n",
    "            r\"\\bmrs\\b\": \"missus \",\n",
    "            r\"\\bst\\b\": \"saint \",\n",
    "            r\"\\bdr\\b\": \"doctor \",\n",
    "            r\"\\bprof\\b\": \"professor \",\n",
    "            r\"\\bcapt\\b\": \"captain \",\n",
    "            r\"\\bgov\\b\": \"governor \",\n",
    "            r\"\\bald\\b\": \"alderman \",\n",
    "            r\"\\bgen\\b\": \"general \",\n",
    "            r\"\\bsen\\b\": \"senator \",\n",
    "            r\"\\brep\\b\": \"representative \",\n",
    "            r\"\\bpres\\b\": \"president \",\n",
    "            r\"\\brev\\b\": \"reverend \",\n",
    "            r\"\\bhon\\b\": \"honorable \",\n",
    "            r\"\\basst\\b\": \"assistant \",\n",
    "            r\"\\bassoc\\b\": \"associate \",\n",
    "            r\"\\blt\\b\": \"lieutenant \",\n",
    "            r\"\\bcol\\b\": \"colonel \",\n",
    "            r\"\\bjr\\b\": \"junior \",\n",
    "            r\"\\bsr\\b\": \"senior \",\n",
    "            r\"\\besq\\b\": \"esquire \",\n",
    "            # prefect tenses, ideally it should be any past participles, but it's harder..\n",
    "            r\"'d been\\b\": \" had been\",\n",
    "            r\"'s been\\b\": \" has been\",\n",
    "            r\"'d gone\\b\": \" had gone\",\n",
    "            r\"'s gone\\b\": \" has gone\",\n",
    "            r\"'d done\\b\": \" had done\",  # \"'s done\" is ambiguous\n",
    "            r\"'s got\\b\": \" has got\",\n",
    "            # general contractions\n",
    "            r\"n't\\b\": \" not\",\n",
    "            r\"'re\\b\": \" are\",\n",
    "            # r\"'s\\b\": \" is\",\n",
    "            r\"'d\\b\": \" would\",\n",
    "            r\"'ll\\b\": \" will\",\n",
    "            r\"'t\\b\": \" not\",\n",
    "            r\"'ve\\b\": \" have\",\n",
    "            r\"'m\\b\": \" am\",\n",
    "        }\n",
    "        self.standardize_numbers = EnglishNumberNormalizer()\n",
    "        self.standardize_spellings = EnglishSpellingNormalizer()\n",
    "\n",
    "    def __call__(self, s: str):\n",
    "        s = s.lower()\n",
    "\n",
    "        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n",
    "        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n",
    "        s = re.sub(self.ignore_patterns, \"\", s)\n",
    "        # s = re.sub(r\"\\s+'\", \"'\", s)  # when there's a space before an apostrophe\n",
    "\n",
    "        for pattern, replacement in self.replacers.items():\n",
    "            s = re.sub(pattern, replacement, s)\n",
    "\n",
    "        s = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", s)  # remove commas between digits\n",
    "        s = re.sub(r\"\\.([^0-9]|$)\", r\" \\1\", s)  # remove periods not followed by numbers\n",
    "        s = remove_symbols_and_diacritics(s, keep=\".%$¢€£-'\")  # keep numeric symbols\n",
    "\n",
    "        # s = self.standardize_numbers(s)\n",
    "        s = self.standardize_spellings(s)\n",
    "\n",
    "        # now remove prefix/suffix symbols that are not preceded/followed by numbers\n",
    "        s = re.sub(r\"[.$¢€£]([^0-9])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"([^0-9])%\", r\"\\1 \", s)\n",
    "\n",
    "        s = re.sub(r\"\\s+\", \" \", s)  # replace any successive whitespaces with a space\n",
    "        \n",
    "        return s\n",
    "normalizer = EnglishTextNormalizer()\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "def gpt_api_no_stream(prompt: str, \n",
    "                      model=\"gpt-4o\",\n",
    "                      reset_messages: bool = True,\n",
    "                      response_only: bool = True\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    ------------\n",
    "    Examples\n",
    "    ------------\n",
    "    \n",
    "    try:\n",
    "        response = gpt_api_no_stream(prompt, model=model)[1]\n",
    "    except AuthenticationError:\n",
    "        continue\n",
    "    if \"OpenAI API error\" in response:\n",
    "        print(f\"{response}\")\n",
    "    else:\n",
    "        np.save(savepath, response)\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"gpt-3.5\" in model:\n",
    "        model = \"gpt-3.5-turbo-1106\"\n",
    "    elif \"gpt-4omini\" in model:\n",
    "        model = \"gpt-4o-mini-2024-07-18\"\n",
    "    elif \"gpt-4o\" in model:\n",
    "        model = \"gpt-4o-2024-11-20\"\n",
    "    elif \"gpt-o1mini\" in model:\n",
    "        model = \"o1-mini-2024-09-12\"\n",
    "    messages = [{'role': 'user','content': prompt},]\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        completion = dict(completion)\n",
    "        msg = None\n",
    "        choices = completion.get('choices', None)\n",
    "        if choices:\n",
    "            msg = choices[0].message.content\n",
    "        else:\n",
    "            msg = completion.message.content\n",
    "    except Exception as err:\n",
    "        return (False, f'OpenAI API error: {err}')\n",
    "    if reset_messages:\n",
    "        messages.pop(-1)\n",
    "    else:\n",
    "        # add text of response to messages\n",
    "        messages.append({\n",
    "            'role': choices[0].message.role,\n",
    "            'content': choices[0].message.content\n",
    "        })\n",
    "    if response_only:\n",
    "        return True, msg\n",
    "    else:\n",
    "        return True, messages\n",
    "\n",
    "def get_json_result(response):\n",
    "    try:\n",
    "        tem = response[::-1][response[::-1].index(\"}\"):][::-1]\n",
    "    \n",
    "        cumulative = \"\"\n",
    "        extra = 1\n",
    "        while extra>0:\n",
    "            extra -= 1\n",
    "            idxb = tem[::-1].index(\"{\")+1\n",
    "            add = tem[::-1][:idxb][::-1]\n",
    "            extra += np.array([a==\"}\" for a in list(add[1:-1])]).sum()\n",
    "            cumulative = add + cumulative\n",
    "            tem = tem[::-1][idxb:][::-1]\n",
    "        curlyblankets = cumulative\n",
    "\n",
    "        # ## Preprocessing\n",
    "        pattern = r\"//.*?\\n\" # delete comment-outs\n",
    "        curlyblankets = re.sub(pattern, \"\", curlyblankets)\n",
    "        l = [] # Delete \"Target Text\" and \"Backchannel or not\"\n",
    "        alsonext = False\n",
    "        for element in curlyblankets.split(\"\\n\"):\n",
    "            if alsonext:\n",
    "                alsonext = False\n",
    "                continue\n",
    "            if \"Target Text\" in element or \"Backchannel\" in element:\n",
    "                if \":\" in element[-2:]:\n",
    "                    alsonext = True\n",
    "                continue\n",
    "            l += [element]\n",
    "        curlyblankets = \"\\n\".join(l)\n",
    "\n",
    "        curlyblankets = curlyblankets.replace(\"null\", '\"neutral\"')\n",
    "        a = eval(curlyblankets)\n",
    "    \n",
    "    except (ValueError, SyntaxError, NameError):\n",
    "        return False, None\n",
    "    \n",
    "    return True, a \n",
    "\n",
    "def GetLLMPrompt(sentence, language, phonemized=None):\n",
    "    words = sentence.split()\n",
    "    if type(phonemized)!=list:\n",
    "        phonemized = [phonemize(word, language='en-us', backend='espeak', with_stress=True).split()[0] for word in words]\n",
    "    shfflephonemized = phonemized\n",
    "\n",
    "    start = f\"\"\"Can you provide me with three {language} words to represent the phoneme sequences delimited by triple backticks. \n",
    "For example, in Japanese, \"Trail (tɹˈeɪl)\" is expected to have Japanese representation of \"トレイル\"; where \"'\" in phonemes represents the stress point of the word. \n",
    "Here, your task is to provide me with three {language} words that can replace the phoneme senquences, delimited by triple backticks.\n",
    "Please focus on phonetically similar characters instead of similar characters in terms of the meaning.\n",
    "The expected output should be in JSON format. \n",
    "You can first list three possible choices of the words and then re-order them in order of the similarity of the pronunciation. \n",
    "The following is the example in Hindi language.\n",
    "{{\n",
    "  \"I\": {{\n",
    "    \"phonemes\": \"ˈaɪ\",\n",
    "    \"choices\": [\"आई\", \"ऐ\", \"आई\"],\n",
    "    \"similarity order\": [\"आई\", \"ऐ\", \"आई\"]\n",
    "  }},\n",
    "  \"love\": {{\n",
    "    \"phonemes\": \"lˈʌv\",\n",
    "    \"choices\": [\"लव\", \"लव\", \"लव\"],\n",
    "    \"similarity order\": [\"लव\", \"लव\", \"लव\"]\n",
    "  }},\n",
    "  \"you\": {{\n",
    "    \"phonemes\": \"juː\",\n",
    "    \"choices\": [\"यू\", \"यू\", \"यू\"],\n",
    "    \"similarity order\": [\"यू\", \"यू\", \"यू\"]\n",
    "  }},\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    for p, ph in enumerate(shfflephonemized):\n",
    "        start += f\"{words[p]}: {ph}\\n\"\n",
    "    start = start[:-1]\n",
    "    start += f\"\"\"\n",
    "```\n",
    "Again, the responses should be in a JSON format and sort them in order of the similarity to each phoneme sequence.\n",
    "{{\n",
    "\"\"\"\n",
    "    for p, ph in enumerate(shfflephonemized):\n",
    "        start += f\"\"\"  \"{words[p]}\": {{\n",
    "\"phonemes\": \"{ph}\",\n",
    "\"choices\": [`1st choices of {language} characters`, `2nd choices of {language} characters`, `3rd choices of {language} characters`],\n",
    "\"similarity order\": [`1st most similar {language} characters`, `2nd most similar {language} characters`, `3rd most similar {language} characters`],\n",
    "}},\\n\"\"\"\n",
    "    start = start[:] + \"}\"\n",
    "    return start\n",
    "\n",
    "adds = {\n",
    "    \"zhi\": [\"the\", [\"ðɪ\"]],\n",
    "    \"za\": [\"the pineapple\", [\"ðə\", \"pˈaɪnæpəl\"]],\n",
    "    \"ah\": [\"a little awkward\", [\"ɐ\",\"lˈɪɾəl\",\"ˈɔːkwɚd\"]],\n",
    "}\n",
    "\n",
    "# Evaluate each word\n",
    "postprocessing = {a: {} for a in adds}\n",
    "for addname in adds:\n",
    "# for addname in [\"zhi\"]:\n",
    "    sentence, phonemized = adds[addname]\n",
    "    # for language in [\"Hindi\", \"Korean\", \"Japanese\", \"Russian\"]:\n",
    "    for language in [\"Hindi\", \"Korean\"]:\n",
    "    # for language in [\"Hindi\", \"Korean\", \"Japanese\"]:\n",
    "        # filelists = glob.glob(f\"./LLM_responses/08-A/{language}/postprocessing_{addname}_*.npy\")\n",
    "        filelists = glob.glob(f\"../../../seq2seq-vc/datasetgeneration/LLM_responses/08-A/{language}/postprocessing_{addname}_*.npy\")\n",
    "        a_list = []\n",
    "        for path in filelists:\n",
    "            response = np.load(path).item()\n",
    "            try:\n",
    "                a_list += [eval(response[response.index(\"{\"):-1*response[::-1].index(\"}\")]) if response[-1]!=\"}\" else eval(response[response.index(\"{\"):])]\n",
    "            except ValueError:\n",
    "                pass\n",
    "        # print(f\"{len(a_list)} / {len(filelists)}\")\n",
    "        # print(\"Normalized    :\", sentence)\n",
    "        dirs = []\n",
    "        for a in a_list:\n",
    "            a = {key: a[key] for key in sentence.split()}\n",
    "            dirs += [a]\n",
    "        for i in range(len(dirs)):\n",
    "            for key in dirs[i]:\n",
    "                newlist = []\n",
    "                for j in range(len(dirs[i][key][\"similarity order\"])):\n",
    "                    newlist += [dirs[i][key][\"similarity order\"][j]]*(3-j)\n",
    "                dirs[i][key][\"similarity order\"] = newlist\n",
    "        data = {key: [element for i in range(len(dirs)) for element in dirs[i][key][\"similarity order\"]] for key in dirs[0]}\n",
    "        # Get the transliterated sentences\n",
    "        arrays = []\n",
    "        counts = []\n",
    "        for word in sentence.split():\n",
    "            c = collections.Counter(data[word])\n",
    "            df = pd.DataFrame(c.items(), columns=[\"phonemes\", \"count\"]).sort_values(\"count\", ascending=False).values\n",
    "            arrays += [df[0,0]]\n",
    "            counts += [list(df[:1,1])]\n",
    "            \n",
    "        postprocessing[addname][language] = arrays[0]\n",
    "\n",
    "def CheckResultValidity(a, inputtext):\n",
    "    if len(a)==len(set(inputtext.split())):\n",
    "        test = []\n",
    "        for word in inputtext.split():\n",
    "            exist = word in a\n",
    "            if not(exist):\n",
    "                normalized_word = normalizer.standardize_numbers(word)\n",
    "                a_array = np.array(list(a.keys()))\n",
    "                bl = normalized_word==a_array\n",
    "                exist = bool(bl.sum())\n",
    "                if exist:\n",
    "                    a[word] = a[normalized_word]\n",
    "            ## check the type of data\n",
    "            if exist:\n",
    "                if type(a[word])!=dict:\n",
    "                    exist = False\n",
    "            test += [exist]\n",
    "        if np.array([test]).mean()==1:\n",
    "            return True, a\n",
    "    return False, None\n",
    "\n",
    "def PostprocessTransliteration(sentence, a_list):\n",
    "    inputtext = normalizer(sentence)\n",
    "    dirs = []\n",
    "    for a in a_list:\n",
    "        a = {key: a[key] for key in inputtext.split()}\n",
    "        dirs += [a]\n",
    "    ordernames = []\n",
    "    for i in range(len(dirs)):\n",
    "        for key in dirs[i]:\n",
    "            newlist = []\n",
    "            try:\n",
    "                ordername = \"similarity order\"\n",
    "                dirs[i][key][ordername]\n",
    "            except KeyError:\n",
    "                ordername = \"similarity_order\"\n",
    "            # delete duplicated words\n",
    "            candidates = list(set(dirs[i][key][ordername]))\n",
    "            newwords = []\n",
    "            for tword in dirs[i][key][ordername]:\n",
    "                if tword in candidates:\n",
    "                    candidates.remove(tword)\n",
    "                    newwords += [tword]\n",
    "                if len(candidates)==0:\n",
    "                    break\n",
    "            dirs[i][key][ordername] = newwords\n",
    "            for j in range(len(dirs[i][key][ordername])):\n",
    "                newlist += [dirs[i][key][ordername][j]]*(len(newwords)-j)\n",
    "                # newlist += [dirs[i][key][ordername][j]]\n",
    "            dirs[i][key][ordername] = newlist\n",
    "        ordernames += [ordername]\n",
    "    data = {key: [element for i in range(len(dirs)) for element in dirs[i][key][ordernames[i]]] for key in dirs[0]}\n",
    "\n",
    "    # Get the transliterated sentences\n",
    "    arrays = []\n",
    "    words = inputtext.split()\n",
    "    for w, word in enumerate(words):\n",
    "        if word in set([a[0].split(\" \")[0] for a in list(adds.values())]):\n",
    "            if word==\"a\":\n",
    "                arrays += [postprocessing[\"ah\"][language]]\n",
    "            if word==\"the\":\n",
    "                pro = phonemize(words[w] + \" \" + words[w+1], language='en-us', backend='espeak', with_stress=True).split()[0]\n",
    "                for the in [\"zhi\", \"za\"]:\n",
    "                    if adds[the][1][0]==pro:\n",
    "                        break\n",
    "                arrays += [postprocessing[the][language]]\n",
    "        else:\n",
    "            c = collections.Counter(data[word])\n",
    "            df = pd.DataFrame(c.items(), columns=[\"phonemes\", \"count\"]).sort_values(\"count\", ascending=False).values\n",
    "            arrays += [df[0,0]]\n",
    "\n",
    "    # put period and comma\n",
    "    try:\n",
    "        targets = [\".\", \",\"]\n",
    "        now = 0\n",
    "        english_arrays = inputtext.split()\n",
    "        for word in sentence.split():\n",
    "        # for word in sentence.split():\n",
    "            normalized = normalizer(word)\n",
    "            num = len(normalized.split(\" \"))\n",
    "            now += (num-1)\n",
    "            for target in targets:\n",
    "                if target in word:\n",
    "                    arrays[now] += target\n",
    "                    english_arrays[now] += target\n",
    "            now += 1\n",
    "    except IndexError:\n",
    "        return None\n",
    "    return \" \".join(arrays)\n",
    "\n",
    "def GetResult(prompt, gptmodel, display_print=False):\n",
    "    repeat = True\n",
    "    trial = 1\n",
    "    while repeat:\n",
    "        response = gpt_api_no_stream(prompt, model=gptmodel)[1]\n",
    "        getresult, a = get_json_result(response)\n",
    "        if getresult:\n",
    "            valid, result = CheckResultValidity(a, inputtext)\n",
    "            if valid:\n",
    "                if display_print:\n",
    "                    print(f\"Trial {trial}: Success!!!\")\n",
    "                repeat = False\n",
    "            else:\n",
    "                if display_print:\n",
    "                    print(f\"Trial {trial}: The result is not valid\")\n",
    "        else:\n",
    "            if display_print:\n",
    "                print(f\"Trial {trial}: Error in Converting Json Format\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "# Trial of Transliteration via LLMs\n",
    "---\n",
    "\n",
    "In this example, we will transliterate an English sentence using a GPT model. Please adjust the following variables:\n",
    "\n",
    "- `sentence`: A string containing the English sentence you wish to transliterate.\n",
    "- `language`: A string specifying the target language. Supported options are \"Hindi\", \"Korean\", and \"Japanese\".\n",
    "- `gptmodel`: A string indicating which GPT model to use. Available options include \"gpt-3.5\", \"gpt-4omini\", \"gpt-4o\", and \"gpt-o1mini\". You can add or modify the list of released models by editing the file `MacST-project-page/sho_util/pyfiles/gpt.py`.\n",
    "\n",
    "Feel free to try out the transliteration with one response from the GPT model using these variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "sentence = \"Transliterate English text into Hindi text.\"\n",
    "language = \"Hindi\"\n",
    "gptmodel = \"gpt-3.5\"\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "inputtext = normalizer(sentence)\n",
    "prompt = GetLLMPrompt(inputtext, language)\n",
    "result = GetResult(prompt, gptmodel)\n",
    "transliterated = PostprocessTransliteration(sentence, [result])\n",
    "\n",
    "print(\"English       :\", sentence)\n",
    "print(\"Normalized    :\", inputtext)\n",
    "print(\"Transliterated:\", transliterated)\n",
    "print(\"\\n----------------------------------------\\n----------------------------------------\\n----------------------------------------\\n\")\n",
    "print(\"PROMPT:\\n\")\n",
    "print(prompt)\n",
    "print(\"\\n----------------------------------------\\n----------------------------------------\\n----------------------------------------\\n\")\n",
    "print(\"Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "# Transliterate Multiple Texts\n",
    "---\n",
    "\n",
    "In this example, we will transliterate multiple English sentences using a GPT model. To improve the reliability of the results, the code generates several transliteration responses for each sentence. Adjust the following variables as needed:\n",
    "\n",
    "- `sentence_list`: A dictionary where each key is a text name and the corresponding value is the English sentence you want to transliterate.\n",
    "- `language`: A string specifying the target language for transliteration. The supported options are \"Hindi\", \"Korean\", and \"Japanese\".\n",
    "- `gptmodel`: A string that indicates which GPT model to use. The available options include \"gpt-3.5\", \"gpt-4omini\", \"gpt-4o\", and \"gpt-o1mini\". You can add or modify the list of models by editing the file `MacST-project-page/sho_util/pyfiles/gpt.py`.\n",
    "- `savedir` : A string that specifies the directory where all transliteration responses will be saved.\n",
    "- `repeatnum`: An integer that sets the number of responses (transliterations) to generate for each sentence.\n",
    "- `reset_response`: A boolean that determines whether to re-generate the transliteration responses, even if previous responses exist in `savedir`.\n",
    "- `transliterated_results`: A dictionary where each key is a text name and the corresponding value is the transliterated text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "sentence_list = {\n",
    "    \"text1\": \"ICASSP in India.\",\n",
    "    \"text2\": \"I'm Sho Inoue.\",\n",
    "}\n",
    "language = \"Hindi\"\n",
    "gptmodel = \"gpt-3.5\"\n",
    "savedir = f\"./responses_{language}_{gptmodel}/\"\n",
    "repeatnum = 3 # Increase this number for more reliable transliteration\n",
    "reset_response = False\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "# Save the valid responses\n",
    "for key in sentence_list:\n",
    "    print(key)\n",
    "    exist_length = len(glob.glob(savedir+f\"{key}_*.npy\"))\n",
    "    if not(reset_response) and exist_length>=repeatnum:\n",
    "        continue\n",
    "    sentence = sentence_list[key]\n",
    "    inputtext = normalizer(sentence)\n",
    "    prompt = GetLLMPrompt(inputtext, language)\n",
    "    \n",
    "    for r in tqdm(range(repeatnum)):\n",
    "        savepath = savedir + f\"{key}_{r}.npy\"\n",
    "        if not(reset_response) and os.path.exists(savepath):\n",
    "            continue\n",
    "        result = GetResult(prompt, gptmodel, display_print=False)\n",
    "        os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "        np.save(savepath, result)\n",
    "\n",
    "transliterated_results = {}\n",
    "for key in sentence_list:\n",
    "    files = glob.glob(savedir+f\"{key}_*.npy\")\n",
    "    transliterated = PostprocessTransliteration(sentence_list[key], [np.load(path, allow_pickle=True).item() for path in files])\n",
    "    transliterated_results[key] = transliterated\n",
    "    \n",
    "for key in sentence_list:\n",
    "    print(\"\\n----------------------------------------\\n----------------------------------------\\n----------------------------------------\\n\")\n",
    "    print(\"Key           :\", key)\n",
    "    print(\"English       :\", sentence_list[key])\n",
    "    print(\"Transliterated:\", transliterated_results[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
